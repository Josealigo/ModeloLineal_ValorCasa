{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# José Ligorría\n",
    "\n",
    "## Regresión Lineal para predecir el valor de un inmueble, tipo casa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La idea es enconrar una expresión sencilla, lineal para describir el valor de una casa en terminos de una única variable y posiblemente una constante de sesgo en el modelo.\n",
    "Por notación llamaremos $y$ al vecor que agrupa los valores del inmueble y a $x$ como la el vector que tiene los valores de la variable con la que se quiere inferir, la cual gracias a un estudio previo (https://github.com/Josealigo/Proyecto1_CDP/blob/master/Proyecto1-Jose_Ligorria.ipynb) se determina que esa variable es 'OverallQual'.\n",
    "Entonces lo que se busca es lo siguiente:\n",
    "\n",
    "$$\\underset{\\alpha,\\beta} argmin \\underset{x_{i} \\in x}{\\sum} \\{ 0.5 * (f(x_{i},\\alpha,\\beta) - y_{i})^{2}) \\}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Donde $$f(x_{i},\\alpha,\\beta) = \\alpha * x_{i} + \\beta$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La metodología a seguir es crear el modelo lineal en código de python, considerando todo de forma vectorizada para hacer uso de las bondades del paquete tensorflow. Utilizando múltiples iteraciones a partir de un vector inicial de parámetros $\\alpha$ y $\\beta$ y corrigiendo para la siguiente iteración dicho vector con el vector gradiente del error, ya que sabemos que una función tiene sus óptimos (locales y globales) para cada punto de la función en dirección del vector gradiente, entonces ese vector de parémetros sería corregido de la siguiente forma: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ (\\alpha,\\beta) = (\\alpha,\\beta) - lr * \\nabla (\\underset{x_{i} \\in x}{\\sum} \\{ 0.5 * (f(x_{i},\\alpha,\\beta) - y_{i})^{2}) \\} )$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El código empleado para éste modelo es el siguiente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import functools\n",
    "from datetime import datetime\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jose-\\Anaconda3\\envs\\Clases_tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\compat\\v2_compat.py:88: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "Enabled compatitility to tf1.x\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.\"):\n",
    "    import tensorflow.compat.v1 as tf\n",
    "    tf.compat.v1.disable_v2_behavior()\n",
    "    tf.compat.v1.disable_eager_execution()\n",
    "    print(\"Enabled compatitility to tf1.x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() \n",
    "def lazy_property(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self, attribute):\n",
    "            setattr(self, attribute, function(self))\n",
    "        return getattr(self, attribute)\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "\n",
    "    def __init__(self, data, target,learning_rate,data_size):\n",
    "        self.data = data\n",
    "        self.target = target\n",
    "        self.learning_rate = learning_rate\n",
    "        self.data_size = data_size\n",
    "        self.m = tf.get_variable(\"parametros\",dtype=tf.float32, shape=[2,1],\n",
    "                    initializer=tf.zeros_initializer())\n",
    "        self.prediction\n",
    "        self.optimize\n",
    "        self.error\n",
    "\n",
    "    @lazy_property\n",
    "    def prediction(self):\n",
    "        tensor_piv = tf.concat([[self.data], [tf.constant(1.0,shape=[self.data_size])]], 0)\n",
    "        pred_piv = tf.linalg.matmul(tensor_piv,self.m,transpose_a=True)\n",
    "        tf.summary.histogram(\"prediccion\", pred_piv)\n",
    "        return pred_piv\n",
    "\n",
    "    @lazy_property\n",
    "    def optimize(self):\n",
    "        gradiente = tf.gradients(self.error ,self.m )\n",
    "        actualizacion_m = tf.assign(self.m, self.m - tf.math.multiply(self.learning_rate, gradiente[0]) )\n",
    "        with tf.name_scope(\"Error\"):\n",
    "            error_piv = self.error\n",
    "            error_summary = tf.summary.scalar(name = \"ErrorSummary\", tensor = error_piv)\n",
    "        return actualizacion_m,error_summary\n",
    "    \n",
    "    \n",
    "\n",
    "    @lazy_property\n",
    "    def error(self):\n",
    "        error_piv = 1/2*tf.reduce_mean(tf.math.square(self.target - self.prediction) )\n",
    "        return error_piv\n",
    "\n",
    "\n",
    "def main(learning_rate,epochs):\n",
    "    data_inicial = np.load('proyecto_training_data.npy')\n",
    "    np.random.seed(2)\n",
    "    sub1 = np.random.choice(range(data_inicial.shape[0]), int(data_inicial.shape[0]*0.08))\n",
    "    sub2 = np.setdiff1d(range(data_inicial.shape[0]),sub1)\n",
    "    data_entrenamiento = data_inicial[sub1]\n",
    "    data_validacion = data_inicial[sub2]\n",
    "    data = tf.placeholder(tf.float32, [len(sub1)])\n",
    "    target = tf.placeholder(tf.float32, [len(sub1)])\n",
    "    model = Model(data, target,learning_rate,len(sub1))\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.initialize_all_variables())\n",
    "    now = datetime.now()\n",
    "    logdir = \"./tf_logs/\" + now.strftime(\"%Y%m%d-%H%M%S\") +\"_lr_\"+str(learning_rate) + \"_epochs_\" + str(epochs)\n",
    "    \n",
    "    train_writer = tf.summary.FileWriter( logdir, sess.graph)\n",
    "\n",
    "    for i in range(epochs):\n",
    "        var_x, var_y = data_entrenamiento[:,1], data_entrenamiento[:,0]\n",
    "        error_piv = sess.run(model.error, {data: var_x, target: var_y})\n",
    "        if (i+1)%math.ceil(epochs/10) ==0:\n",
    "            print(\"Error \", i+1, \": \",error_piv)\n",
    "        optimizador = sess.run(model.optimize, {data: var_x, target: var_y})\n",
    "        merge = tf.summary.merge_all()\n",
    "        train_writer.add_summary(optimizador[1],i)\n",
    "    train_writer.close()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El modelo anterior genera el siguiente grafo en la metodología Tensorboard:\n",
    "<img src=\"Imagenes/Grafo.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\jose-\\Anaconda3\\envs\\Clases_tensorflow_env\\lib\\site-packages\\tensorflow_core\\python\\util\\tf_should_use.py:235: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Error  30 :  inf\n",
      "Error  60 :  inf\n",
      "Error  90 :  nan\n",
      "Error  120 :  nan\n",
      "Error  150 :  nan\n",
      "Error  180 :  nan\n",
      "Error  210 :  nan\n",
      "Error  240 :  nan\n",
      "Error  270 :  nan\n",
      "Error  300 :  nan\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.1,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 1:\n",
    "    Donde vemos que el learnig rate de 0.1 es ineficiente entonces hay que cambiarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  30 :  2643587300.0\n",
      "Error  60 :  2630591700.0\n",
      "Error  90 :  2617889800.0\n",
      "Error  120 :  2605475800.0\n",
      "Error  150 :  2593343500.0\n",
      "Error  180 :  2581484800.0\n",
      "Error  210 :  2569896400.0\n",
      "Error  240 :  2558569200.0\n",
      "Error  270 :  2547499300.0\n",
      "Error  300 :  2536680400.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.01,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 2:\n",
    "    Donde vemos que el learnig rate de 0.01 tiene resultados positivos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  30 :  4049424000.0\n",
      "Error  60 :  2772107000.0\n",
      "Error  90 :  2662503200.0\n",
      "Error  120 :  2651984000.0\n",
      "Error  150 :  2649875000.0\n",
      "Error  180 :  2648480000.0\n",
      "Error  210 :  2647153000.0\n",
      "Error  240 :  2645829600.0\n",
      "Error  270 :  2644511200.0\n",
      "Error  300 :  2643196700.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph()\n",
    "    main(0.001,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 3:\n",
    "    Donde podemos ver que el learning rate de 0.01 tiene mejores resultados por lo que 0.001 entonces procedemos probando con 0.03."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  30 :  2618715000.0\n",
      "Error  60 :  2582241500.0\n",
      "Error  90 :  2548192800.0\n",
      "Error  120 :  2516408000.0\n",
      "Error  150 :  2486738200.0\n",
      "Error  180 :  2459038500.0\n",
      "Error  210 :  2433183000.0\n",
      "Error  240 :  2409046300.0\n",
      "Error  270 :  2386513700.0\n",
      "Error  300 :  2365482500.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.03,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 4:\n",
    "    Donde vemos que el aprendizaje obtenido con la misma cantidad de Epochs es mejor que los anteriores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  30 :  36567888000.0\n",
      "Error  60 :  80951580000.0\n",
      "Error  90 :  183474440000.0\n",
      "Error  120 :  420180660000.0\n",
      "Error  150 :  966586600000.0\n",
      "Error  180 :  2227809000000.0\n",
      "Error  210 :  5138880700000.0\n",
      "Error  240 :  11858009000000.0\n",
      "Error  270 :  27366570000000.0\n",
      "Error  300 :  63162310000000.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.05,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 5:\n",
    "    Al tener un learning rate de 0.05 el error crece demasiado, por lo que probamos con 0.04."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  30 :  2606676500.0\n",
      "Error  60 :  2559639800.0\n",
      "Error  90 :  2516727300.0\n",
      "Error  120 :  2477575700.0\n",
      "Error  150 :  2441857000.0\n",
      "Error  180 :  2409269800.0\n",
      "Error  210 :  2379541000.0\n",
      "Error  240 :  2352417500.0\n",
      "Error  270 :  2327672000.0\n",
      "Error  300 :  2305096400.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.04,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 6:\n",
    "    Vemos que para 0.04 tenemos hasta ahora la mejor optimización del error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  30 :  2600848000.0\n",
      "Error  60 :  2548713000.0\n",
      "Error  90 :  2501774300.0\n",
      "Error  120 :  2459440600.0\n",
      "Error  150 :  2421259000.0\n",
      "Error  180 :  2386820600.0\n",
      "Error  210 :  2355764500.0\n",
      "Error  240 :  2327750400.0\n",
      "Error  270 :  2302487000.0\n",
      "Error  300 :  2279699700.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.045,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 7:\n",
    "    Vemos que para 0.045 obtenemos una mejora en la optimización."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  30 :  5832955400.0\n",
      "Error  60 :  3196544000.0\n",
      "Error  90 :  2623283200.0\n",
      "Error  120 :  2472503300.0\n",
      "Error  150 :  2411075000.0\n",
      "Error  180 :  2371045000.0\n",
      "Error  210 :  2338286000.0\n",
      "Error  240 :  2309620500.0\n",
      "Error  270 :  2284129300.0\n",
      "Error  300 :  2261373000.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.049,300)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 8:\n",
    "    Vemos que para 0.049 podemos mejorar un poco más la optimización. Por lo que ahora procedemos a ver si los dos mejores valores de learning rate pueden mejorar con más iteraciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  100 :  2552929300.0\n",
      "Error  200 :  2348673800.0\n",
      "Error  300 :  2261373000.0\n",
      "Error  400 :  2201616600.0\n",
      "Error  500 :  2160538400.0\n",
      "Error  600 :  2132297300.0\n",
      "Error  700 :  2112884400.0\n",
      "Error  800 :  2099536900.0\n",
      "Error  900 :  2090361900.0\n",
      "Error  1000 :  2084055300.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.049,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  100 :  2487177200.0\n",
      "Error  200 :  2365762600.0\n",
      "Error  300 :  2279699700.0\n",
      "Error  400 :  2218695200.0\n",
      "Error  500 :  2175454000.0\n",
      "Error  600 :  2144802700.0\n",
      "Error  700 :  2123073700.0\n",
      "Error  800 :  2107673300.0\n",
      "Error  900 :  2096756000.0\n",
      "Error  1000 :  2089017900.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.045,1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 9:\n",
    "    Vemos que para el learning rate de 0.049 se presentan los valores de error más pequeños. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error  600 :  2132297300.0\n",
      "Error  1200 :  2076736500.0\n",
      "Error  1800 :  2070871600.0\n",
      "Error  2400 :  2070253600.0\n",
      "Error  3000 :  2070187000.0\n",
      "Error  3600 :  2070179800.0\n",
      "Error  4200 :  2070179800.0\n",
      "Error  4800 :  2070179600.0\n",
      "Error  5400 :  2070179000.0\n",
      "Error  6000 :  2070178200.0\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph() \n",
    "    main(0.049,6000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Experimento 10:\n",
    "    Vemos que para el learning rate de 0.049 se presentan los valores de error más pequeños. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo cual genera las siguiente gráfica de errores, apoyado en Tensorboard:\n",
    "<img src=\"Imagenes/Errores.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Por otro lado, si suponemos la estimación para el valor de una casa como el promedio del valo de todas las casas tenemos el siguiente error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2070179531.6879828"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_inicial = np.load('proyecto_training_data.npy')\n",
    "np.random.seed(2)\n",
    "sub1 = np.random.choice(range(data_inicial.shape[0]), int(data_inicial.shape[0]*0.08))\n",
    "sub2 = np.setdiff1d(range(data_inicial.shape[0]),sub1)\n",
    "data_entrenamiento = data_inicial[sub1]\n",
    "0.5*np.mean((data_entrenamiento[:,0]-np.mean(data_entrenamiento[:,0]))**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusión\n",
    "\n",
    "Dada la experimentación presentada se tiene que los mejores resultados los obtiene el learning rate de 0.049 y después de 6000 iteraciones se converge a un error de 2,070,178,200.0 en comparación con el estimado usando el promedio que es de 2,070,179,531.0 vemos que no es diferente, por lo que la estimación de ésta forma sujeto a los valores de entrenamiento no tiene un alto grado de acierto, menos se esperaría si se prueba con la información de prueba. Dadas esas consideraciones se tiene que la hipótesis de estimar el valor de la casa con la variable OverallQual en un modelo lineal no tiene un buen ajuste."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
